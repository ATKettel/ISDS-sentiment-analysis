{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62d101-0d46-455e-88b2-7508f0c4a200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93f2ba81-c241-4991-a2e0-1a5878e0f9c0",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c3cd63-7776-4a7b-a525-bffc80e7ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3356d-46e3-404f-8a24-264e6c117f16",
   "metadata": {},
   "source": [
    "# Importing DR and Berlingske texts and merging them into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8211d284-be69-41cb-b8f6-daa4e2f2b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_DR = pd.read_csv('DR_full_data.csv')\n",
    "data_DR['source'] = 'dr.dk' # adding a source variable\n",
    "\n",
    "data_B = pd.read_csv('Berlingske_full_data.csv')\n",
    "data_B['source'] = 'berlingske.dk' # adding a source variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27140e33-70be-48ed-9f00-a12c7422160a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Making sure we only keep relevant columns and giving them the same names:\n",
    "data_B = data_B.rename(columns={'Title': 'title', 'Date': 'date', 'Content':'article_text', 'URL':'url'})\n",
    "\n",
    "data_DR = data_DR.rename(columns = {'Title': 'title', 'Date': 'date', 'Content':'article_text', 'URL':'url'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90310796-9377-4616-9724-ca69afe3d66f",
   "metadata": {},
   "source": [
    "**Converting the dates to be in the same format:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822ccda-f339-493f-b7c8-01dcc3221239",
   "metadata": {},
   "source": [
    "*Berlingske*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1f2ac95-b1bd-45e7-a1f3-08f556196cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Berlingske: \n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract the day, month name, and year parts using regex and process in place\n",
    "data_B[['day', 'month', 'year']] = data_B['date'].str.extract(r'd. (\\d{2})\\. (\\w+) (\\d{4})')\n",
    "\n",
    "# Step 2: Convert the month names to month numbers (Danish to English)\n",
    "month_mapping = {\n",
    "    'januar': '01', 'februar': '02', 'marts': '03', 'april': '04',\n",
    "    'maj': '05', 'juni': '06', 'juli': '07', 'august': '08',\n",
    "    'september': '09', 'oktober': '10', 'november': '11', 'december': '12'\n",
    "}\n",
    "data_B['month'] = data_B['month'].map(month_mapping)\n",
    "\n",
    "# Step 3: Combine the cleaned date information and update the 'date' column\n",
    "data_B['date'] = data_B['year'] + '-' + data_B['month'] + '-' + data_B['day']\n",
    "\n",
    "# Step 4: Convert the 'date' column to datetime format\n",
    "data_B['date'] = pd.to_datetime(data_B['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Drop the extra columns used for processing ('day', 'month', 'year')\n",
    "data_B.drop(columns=['day', 'month', 'year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91263f1f-3509-4b60-afb2-88175b3bf8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making it datetime: \n",
    "data_B['date'] = pd.to_datetime(data_B['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5620c61-c06a-4a4d-b53b-f93dab430454",
   "metadata": {},
   "source": [
    "*DR*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6a3c956-1819-406f-a16c-a2f9cf4efbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DR also has the time included, which confuses when trying to convert to datetime format, so we will remove those first:\n",
    "data_DR['date'] = data_DR['date'].str.replace(r'T.*', '', regex=True)\n",
    "data_DR['date'] = pd.to_datetime(data_DR['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43dc16a2-ff09-4783-b19b-168a6bca3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing NAs:\n",
    "data_DR = data_DR.dropna()\n",
    "data_B = data_B.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a33e582e-71b5-4b57-bf47-ea93003edd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging them into one:\n",
    "data = pd.concat([data_DR, data_B], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60762bf7-33ef-4dba-b744-8df405bce18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'article_date' column from oldest to newest\n",
    "data = data.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9a3c1fb-940a-4a29-a0d5-b0fc8f5c671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving as a complete CSV: \n",
    "data.to_csv('DK_full_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944704e1-05e9-4dc1-912d-c9689d07a39e",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e9f322a-c5eb-4061-b35e-ab6d442874a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the full DK data file\n",
    "data = pd.read_csv('DK_full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "444fcb0e-508d-4a8f-a601-509d382e4051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>article_text</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.berlingske.dk/business/mobilgigant...</td>\n",
       "      <td>Lykketræf: Den tyske industrigigant Siemens er...</td>\n",
       "      <td>2002-07-14</td>\n",
       "      <td>berlingske.dk</td>\n",
       "      <td>Mobilgiganten fra München</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.dr.dk/nyheder/penge/farvel-til-kfx...</td>\n",
       "      <td>I dag må investorer, der er interesserede i de...</td>\n",
       "      <td>2005-10-03</td>\n",
       "      <td>dr.dk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dr.dk/nyheder/penge/fald-i-omxc20-...</td>\n",
       "      <td>Det var en sur sidste dag i ugen, hvor markede...</td>\n",
       "      <td>2005-10-21</td>\n",
       "      <td>dr.dk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.dr.dk/nyheder/penge/novo-var-lyset...</td>\n",
       "      <td>I dag var igen en forholdsvis hektisk dag på d...</td>\n",
       "      <td>2005-10-27</td>\n",
       "      <td>dr.dk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dr.dk/nyheder/penge/tyve-stoerste-...</td>\n",
       "      <td>Der hersker mandag morgen optimisme på de euro...</td>\n",
       "      <td>2005-10-31</td>\n",
       "      <td>dr.dk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12941</th>\n",
       "      <td>https://www.berlingske.dk/aktier/en-tastefejl-...</td>\n",
       "      <td>Flere og flere handler på aktiemarkedet bliver...</td>\n",
       "      <td>2024-08-12</td>\n",
       "      <td>berlingske.dk</td>\n",
       "      <td>En tastefejl udløste drastisk kursfald. Aktiem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12942</th>\n",
       "      <td>https://www.berlingske.dk/business/business-ov...</td>\n",
       "      <td>Her er de vigtigste Business-nyheder, som du s...</td>\n",
       "      <td>2024-08-13</td>\n",
       "      <td>berlingske.dk</td>\n",
       "      <td>Business-overblik: Sidste uges aktiepanik er f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12943</th>\n",
       "      <td>https://www.berlingske.dk/oekonomi/groenne-tal...</td>\n",
       "      <td>Aktiemarkedet kom skidt fra start i august, hv...</td>\n",
       "      <td>2024-08-15</td>\n",
       "      <td>berlingske.dk</td>\n",
       "      <td>Grønne tal og optimisme dominerer aktierne. Ek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12944</th>\n",
       "      <td>https://www.berlingske.dk/business/bavarian-no...</td>\n",
       "      <td>Data fra et forsøg viser ifølge Bavarian Nordi...</td>\n",
       "      <td>2024-08-16</td>\n",
       "      <td>berlingske.dk</td>\n",
       "      <td>Bavarian Nordic vil have mpox-vaccine godkendt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12945</th>\n",
       "      <td>https://www.berlingske.dk/virksomheder/revisor...</td>\n",
       "      <td>Landets revisorhuse kæmper hårdt om at vinde d...</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>berlingske.dk</td>\n",
       "      <td>Revisorhus fører kampen om lukrative storkunde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12946 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  \\\n",
       "0      https://www.berlingske.dk/business/mobilgigant...   \n",
       "1      https://www.dr.dk/nyheder/penge/farvel-til-kfx...   \n",
       "2      https://www.dr.dk/nyheder/penge/fald-i-omxc20-...   \n",
       "3      https://www.dr.dk/nyheder/penge/novo-var-lyset...   \n",
       "4      https://www.dr.dk/nyheder/penge/tyve-stoerste-...   \n",
       "...                                                  ...   \n",
       "12941  https://www.berlingske.dk/aktier/en-tastefejl-...   \n",
       "12942  https://www.berlingske.dk/business/business-ov...   \n",
       "12943  https://www.berlingske.dk/oekonomi/groenne-tal...   \n",
       "12944  https://www.berlingske.dk/business/bavarian-no...   \n",
       "12945  https://www.berlingske.dk/virksomheder/revisor...   \n",
       "\n",
       "                                            article_text        date  \\\n",
       "0      Lykketræf: Den tyske industrigigant Siemens er...  2002-07-14   \n",
       "1      I dag må investorer, der er interesserede i de...  2005-10-03   \n",
       "2      Det var en sur sidste dag i ugen, hvor markede...  2005-10-21   \n",
       "3      I dag var igen en forholdsvis hektisk dag på d...  2005-10-27   \n",
       "4      Der hersker mandag morgen optimisme på de euro...  2005-10-31   \n",
       "...                                                  ...         ...   \n",
       "12941  Flere og flere handler på aktiemarkedet bliver...  2024-08-12   \n",
       "12942  Her er de vigtigste Business-nyheder, som du s...  2024-08-13   \n",
       "12943  Aktiemarkedet kom skidt fra start i august, hv...  2024-08-15   \n",
       "12944  Data fra et forsøg viser ifølge Bavarian Nordi...  2024-08-16   \n",
       "12945  Landets revisorhuse kæmper hårdt om at vinde d...  2024-08-19   \n",
       "\n",
       "              source                                              title  \n",
       "0      berlingske.dk                          Mobilgiganten fra München  \n",
       "1              dr.dk                                                NaN  \n",
       "2              dr.dk                                                NaN  \n",
       "3              dr.dk                                                NaN  \n",
       "4              dr.dk                                                NaN  \n",
       "...              ...                                                ...  \n",
       "12941  berlingske.dk  En tastefejl udløste drastisk kursfald. Aktiem...  \n",
       "12942  berlingske.dk  Business-overblik: Sidste uges aktiepanik er f...  \n",
       "12943  berlingske.dk  Grønne tal og optimisme dominerer aktierne. Ek...  \n",
       "12944  berlingske.dk  Bavarian Nordic vil have mpox-vaccine godkendt...  \n",
       "12945  berlingske.dk  Revisorhus fører kampen om lukrative storkunde...  \n",
       "\n",
       "[12946 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8b5ca-6e97-4999-a8d1-086b2d53d802",
   "metadata": {},
   "source": [
    "## What is your document?\n",
    "\n",
    "Our document is the article as a whole - all text in the article excluding image discriptions, and authour tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d23d9-596c-425b-8fbd-11552e9a07bc",
   "metadata": {},
   "source": [
    "## Preprossesing\n",
    "- Clean text: ignore/remove any unwanted characters: casing, HTML markup, non-words, etc. (maybe also emoticons?)\n",
    "- Tokenization and stop-words\n",
    "- Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a32fcc-d723-4eef-9698-9edb1e9d994c",
   "metadata": {},
   "source": [
    "***Removing NA values*** on the column \"article_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a0db6afa-96c3-43c1-b1b5-b2aa41191578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values in the 'article_text' column\n",
    "data = data.dropna(subset=['article_text'])\n",
    "data = data.reset_index(drop=True)  # Reset the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448986e-443c-4263-bd17-d56ff0e50c79",
   "metadata": {},
   "source": [
    "***Cleaning the text***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b410af0-8429-4c88-8402-985402104c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(document):\n",
    "    document = document.lower()  # To lower case\n",
    "    document = re.sub(r'<[^>]*>', ' ', document)  # Remove HTML\n",
    "    document = re.sub(r'[^\\w\\s&$€%]', '', document)  # Remove non-alphanumeric characters except &, $, %, and €\n",
    "    return document\n",
    "\n",
    "data['article_text'] = data['article_text'].apply(cleaner)\n",
    "\n",
    "# Checking for duplicates: \n",
    "data[data['article_text'].duplicated()]\n",
    "\n",
    "# Removing the duplicates articles: \n",
    "data = data.drop_duplicates(subset=['article_text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23f48d-defa-4948-bb8b-f2ccf1c2baf9",
   "metadata": {},
   "source": [
    "***Tokenization***\n",
    "- Splitting the articles into meaningfull elements to prepare for analysis. In our case we need to split the articles into words as these are what will be used for classifying sentiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50b7495f-beba-467a-9a6d-9bdec89dfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'article_text' column into tokens based on whitespace and saving it into a new column\n",
    "# \"words\"\n",
    "data['words'] = data['article_text'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd6baa4-3fbf-4f8a-88fe-64a085b31f33",
   "metadata": {},
   "source": [
    "***Removing stop words***\n",
    "- These are words that occur very often and probably bear no useful information about the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59f22730-c8de-4f84-8230-1229842c3e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/astakettel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = nltk.corpus.stopwords.words('danish')\n",
    "data['words'] = [i for i in data['words'] if i not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d171b48-5e26-4684-a150-a4270139f06e",
   "metadata": {},
   "source": [
    "***Saving as a CSV file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4bfb5b41-8b48-461f-89a3-b0868dc7bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('DK_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09149b15-605a-474d-a410-a2665a240140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f3f2dfb-4751-419f-b74f-58e81554ae8e",
   "metadata": {},
   "source": [
    "# Using the lexicon vader for sentiment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc17afb-f265-4e48-837f-f03918bb03ad",
   "metadata": {},
   "source": [
    "We need to adjust the VADER sentiment analysis technique for Danish. The following is a step by step guide that i can also use to describe the process in the final paper. I start by downloading the VADER lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48646046-dcf2-4c2c-a2fa-6fd1fd359d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/astakettel/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43e2bc-0112-4659-9967-d348700bbab3",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the Danish Sentiment Lexicon\n",
    "Then i want to convert the Danish sentimenr lexicon to a format that is compatible with VADER: The VADEr lexicon is a dictionary where each word is associated with a sentiment score. \n",
    "\n",
    "I have downloaded te Danish sentiment lexicon as a CSV file from the github repository; https://github.com/dsldk/danish-sentiment-lexicon\n",
    "We need to clean the Danish sentiment lexicon, to ensure that both the base words and their various forms are included in the lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "241a75f8-2f8e-4fbd-96ce-d7f8d82c16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "#Loading \n",
    "danish_lexicon_df = pd.read_csv('/Users/astakettel/Desktop/ISDS/GitHub/ISDS-sentiment-analysis/Danish sentiments/2_headword_headword_polarity.csv', \n",
    "                               header = None)\n",
    "# renaming the columns for clarity:\n",
    "danish_lexicon_df.columns = ['word', 'column_2', 'part_of_speech', 'code', 'polarity', 'forms']\n",
    "\n",
    "# Convert the polirarity column to numeric type: \n",
    "danish_lexicon_df['polarity'] = pd.to_numeric(danish_lexicon_df['polarity'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing or invalid data in 'word' or 'polarity'\n",
    "lexicon_cleaned = danish_lexicon_df.dropna(subset=['word', 'polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6874f-dbbc-44c6-bd10-eccd8262eb8c",
   "metadata": {},
   "source": [
    "#### Rescaling the Danish lexicon to match the VADER scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0ec80-c736-4f23-9fde-cc6f774c5aca",
   "metadata": {},
   "source": [
    "The original scale of the sentiments in the Danish lexicon is: = \"-5\" (maximum degree negative)|\"-4\" (very high degree negative)|\"-3\" (high degree negative) |\"-2\" (degree negative) | \"-1\" (low degree negative) | \"1\" (low degree positive)| \"2\" (degree positive)|\"3\" (high degree positive)|\"4\" (very high degree positive)| \"5\" (maximum degree positive)\n",
    "\n",
    "Therefore we need to rescalethe Danish polarity to VADER scale: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "78246cd6-6cb3-484f-97ca-e8296c899057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rescale Danish polarity to VADER scale\n",
    "def rescale_polarity(danish_polarity):\n",
    "    # Rescale from -5 to +5 range to -4 to +4 range\n",
    "    vader_polarity = ((danish_polarity - (-5)) / (5 - (-5))) * (4 - (-4)) + (-4)\n",
    "    return vader_polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cad248-4661-4457-8d79-42c4657c2c84",
   "metadata": {},
   "source": [
    "Then we make a dictonary to store all words - both base form and other forms of each word - and their associated poliarity scores- The scores are rescaled to match the VADEr scores, using the function we just defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ef907db1-d33a-44eb-bef5-f68e24bf169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold all words and their respective rescaled sentiment scores\n",
    "danish_lexicon = {}\n",
    "\n",
    "# Include all word forms and base words in the lexicon\n",
    "for index, row in lexicon_cleaned.iterrows():\n",
    "    # Rescale the Danish polarity to match VADER's scale\n",
    "    vader_polarity = rescale_polarity(row['polarity'])\n",
    "    \n",
    "    # Add the base word with its rescaled polarity\n",
    "    danish_lexicon[row['word']] = vader_polarity\n",
    "    \n",
    "    # Add all conjugated forms of the word to the lexicon, if available\n",
    "    if pd.notna(row['forms']):\n",
    "        # Split the forms by semicolon, assuming they are separated this way\n",
    "        forms = row['forms'].split(';')\n",
    "        for form in forms:\n",
    "            # Ensure all forms have the same rescaled sentiment score as the base word\n",
    "            danish_lexicon[form] = vader_polarity\n",
    "\n",
    "# Now, danish_lexicon contains the headword and all its forms with rescaled polarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bade22c-a7dc-4c43-a104-1feabade99f6",
   "metadata": {},
   "source": [
    "### Step 3 Modify VADER with the Danish Lexicon\n",
    "Then we integrate the Danish lexicon into VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "320b998a-33fc-4890-977a-d8d3db8c4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Update VADER's lexicon with the Danish lexicon (including all forms and rescaled values)\n",
    "analyser.lexicon.update(danish_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25121fbe-09e6-4eaf-ab84-95eeb3d0accc",
   "metadata": {},
   "source": [
    "### Step 4: Performing the sentiment analysis\n",
    "\n",
    "I start bt defining a function that calculates the compound sentiment score for each row of text in a Dataframe using the polarity_scores() method. \n",
    "And then i apply this function to each article in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a2b9967d-6a76-44dc-87c5-f5340982afa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            article_text  sentiment\n",
      "0      lykketræf den tyske industrigigant siemens er ...     0.8225\n",
      "1      i dag må investorer der er interesserede i de ...     0.7783\n",
      "2      det var en sur sidste dag i ugen hvor markedet...    -0.9274\n",
      "3      i dag var igen en forholdsvis hektisk dag på d...     0.9906\n",
      "4      der hersker mandag morgen optimisme på de euro...     0.9274\n",
      "...                                                  ...        ...\n",
      "12814  flere og flere handler på aktiemarkedet bliver...     0.9964\n",
      "12815  her er de vigtigste businessnyheder som du ska...    -0.9042\n",
      "12816  aktiemarkedet kom skidt fra start i august hvo...     0.9866\n",
      "12817  data fra et forsøg viser ifølge bavarian nordi...     0.9501\n",
      "12818  landets revisorhuse kæmper hårdt om at vinde d...     0.9988\n",
      "\n",
      "[12819 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to get the compound sentiment score\n",
    "def get_sentiment(text):\n",
    "    # Get the compound sentiment score\n",
    "    return analyser.polarity_scores(text)['compound']\n",
    "\n",
    "# Apply the function to the 'article_text' column\n",
    "data['sentiment'] = data['article_text'].apply(get_sentiment)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(data[['article_text', 'sentiment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b7182-a18f-44f4-bd05-9aadd06d8930",
   "metadata": {},
   "source": [
    "In VADER sentiment analysis the compound score is a normalized weighted composite score which represnt the overall sentiment of a given text. It takes into account the positive, negative and neutral score calculated by VADER and combines them into a single value, which provide the overall sentiment rating.\n",
    "\n",
    "- The compound score is calculated as a weighhted sum of the valence scores of each word in the text, normalized to fall within the -1 to +q range. VADER uses heuritics to balnce the impact of negation, punctuation (like !) and intensifiers (like \"very\")\n",
    "- The compound score ranges from -1 to +1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9bb48-ea9f-4673-aef5-283b184f4b4d",
   "metadata": {},
   "source": [
    "## Computing sentiment scores from -1 to 1 and binary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84070bde-5630-4d3d-b88c-e0626a8809d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column using an if-else loop to classify sentiment\n",
    "data['pos/neg'] = ''\n",
    "\n",
    "# Loop through each row and assign the sentiment label\n",
    "for i in range(len(data)):\n",
    "    if data.loc[i, 'sentiment'] > 0:\n",
    "        data.loc[i, 'pos/neg'] = 'positive'\n",
    "    else:\n",
    "        data.loc[i, 'pos/neg'] = 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902509c6-d2e4-482d-adc2-273ec8ae7b37",
   "metadata": {},
   "source": [
    "## Saving as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e8bc2c64-0ac8-440b-8c6c-5ba6cf9da9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Users/astakettel/Desktop/ISDS/GitHub/ISDS-sentiment-analysis/Webscraping/DK_clean_sent.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
