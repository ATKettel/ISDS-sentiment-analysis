{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62d101-0d46-455e-88b2-7508f0c4a200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93f2ba81-c241-4991-a2e0-1a5878e0f9c0",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1c3cd63-7776-4a7b-a525-bffc80e7ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3356d-46e3-404f-8a24-264e6c117f16",
   "metadata": {},
   "source": [
    "# Importing DR and Berlingske texts and merging them into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211d284-be69-41cb-b8f6-daa4e2f2b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_DR = pd.read_csv('DR_full_data.csv')\n",
    "data_DR['source'] = 'dr.dk' # adding a source variable\n",
    "\n",
    "data_B = pd.read_csv('Berlingske_full_data.csv')\n",
    "data_B['source'] = 'berlingske.dk' # adding a source variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27140e33-70be-48ed-9f00-a12c7422160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! MODIFY !!! \n",
    "# Making sure we only keep relevant columns and giving them the same names:\n",
    "data_CNN = data_CNN[['headline', 'url', 'lastModifiedDate','article_text','source']]\n",
    "data_CNN = data_CNN.rename(columns={'headline': 'title', 'lastModifiedDate': 'date'})\n",
    "\n",
    "data_ABC = data_ABC.rename(columns = {'URL':'url', 'Title':'title', 'Date':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a3c956-1819-406f-a16c-a2f9cf4efbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! MODIFY !!!\n",
    "# Convert to date-time format \n",
    "\n",
    "# DR also has the time included, which confuses when trying to convert to datetime format, so we will remove those first:\n",
    "data_DR['date'] = data_DR['date'].str.replace(r'T.*', '', regex=True)\n",
    "data_DR['date'] = pd.to_datetime(data_DR['date'], errors='coerce')\n",
    "\n",
    "\n",
    "# Berlingske: check how the datetime is formatted before\n",
    "data_ABC['date'] = pd.to_datetime(data_ABC['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e582e-71b5-4b57-bf47-ea93003edd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging them into one:\n",
    "data = pd.concat([data_DR, data_B], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60762bf7-33ef-4dba-b744-8df405bce18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'article_date' column from oldest to newest\n",
    "data = data.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944704e1-05e9-4dc1-912d-c9689d07a39e",
   "metadata": {},
   "source": [
    "# While we wait for the Berlingske articles, i just try with the DR.dk articles: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e9f322a-c5eb-4061-b35e-ab6d442874a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE THIS AFTER BERLINGSKE IS READY \n",
    "data = pd.read_csv('DR_full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e5c43f4-7d1c-430c-ac9b-8e5c53b80dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = data['date'].str.replace(r'T.*', '', regex=True)\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8b5ca-6e97-4999-a8d1-086b2d53d802",
   "metadata": {},
   "source": [
    "## What is your document?\n",
    "\n",
    "Our document is the article as a whole - all text in the article excluding image discriptions, and authour tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d23d9-596c-425b-8fbd-11552e9a07bc",
   "metadata": {},
   "source": [
    "## Preprossesing\n",
    "- Clean text: ignore/remove any unwanted characters: casing, HTML markup, non-words, etc. (maybe also emoticons?)\n",
    "- Tokenization and stop-words\n",
    "- Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a32fcc-d723-4eef-9698-9edb1e9d994c",
   "metadata": {},
   "source": [
    "***Removing NA values*** on the column \"article_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0db6afa-96c3-43c1-b1b5-b2aa41191578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values in the 'article_text' column\n",
    "data = data.dropna(subset=['article_text'])\n",
    "data = data.reset_index(drop=True)  # Reset the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448986e-443c-4263-bd17-d56ff0e50c79",
   "metadata": {},
   "source": [
    "***Cleaning the text***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b410af0-8429-4c88-8402-985402104c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(document):\n",
    "    document = document.lower()  # To lower case\n",
    "    document = re.sub(r'<[^>]*>', ' ', document)  # Remove HTML\n",
    "    document = re.sub(r'[^\\w\\s&$€%]', '', document)  # Remove non-alphanumeric characters except &, $, %, and €\n",
    "    return document\n",
    "\n",
    "data['article_text'] = data['article_text'].apply(cleaner)\n",
    "\n",
    "# Checking for duplicates: \n",
    "data[data['article_text'].duplicated()]\n",
    "\n",
    "# Removing the duplicates articles: \n",
    "data = data.drop_duplicates(subset=['article_text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23f48d-defa-4948-bb8b-f2ccf1c2baf9",
   "metadata": {},
   "source": [
    "***Tokenization***\n",
    "- Splitting the articles into meaningfull elements to prepare for analysis. In our case we need to split the articles into words as these are what will be used for classifying sentiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50b7495f-beba-467a-9a6d-9bdec89dfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'article_text' column into tokens based on whitespace and saving it into a new column\n",
    "# \"words\"\n",
    "data['words'] = data['article_text'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd6baa4-3fbf-4f8a-88fe-64a085b31f33",
   "metadata": {},
   "source": [
    "***Removing stop words***\n",
    "- These are words that occur very often and probably bear no useful information about the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59f22730-c8de-4f84-8230-1229842c3e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/astakettel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = nltk.corpus.stopwords.words('danish')\n",
    "data['words'] = [i for i in data['words'] if i not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d171b48-5e26-4684-a150-a4270139f06e",
   "metadata": {},
   "source": [
    "***Saving as a CSV file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb5b41-8b48-461f-89a3-b0868dc7bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('DK_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
