{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5091a745-93af-4d40-99a4-d8c97ff97757",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6b60b6-8bbb-4c22-a7e5-916c81dfe495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ead705-c782-440e-a6f8-3fb5670cea8a",
   "metadata": {},
   "source": [
    "# Importing articles from ABCnews and CNN and merging them into one: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d92e63-f52d-4a81-9658-cc0fb3c9c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ABC = pd.read_csv('ABC_full_data.csv')\n",
    "\n",
    "data_CNN = pd.read_csv('CNN_full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb63dfdb-954f-4fec-98ed-c9f8c46961cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure we only keep relevant columns and giving them the same names:\n",
    "data_CNN = data_CNN[['headline', 'url', 'lastModifiedDate','article_text','source']]\n",
    "data_CNN = data_CNN.rename(columns={'headline': 'title', 'lastModifiedDate': 'date'})\n",
    "\n",
    "data_ABC = data_ABC.rename(columns = {'URL':'url', 'Title':'title', 'Date':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5d1a4-f43d-41b9-9b07-c329ee345ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the Date columns to be in the same format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8bf09d-c171-491b-8e62-a102a6f712f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CNN also has the time included, which confuses when trying to convert to datetime format, so we will remove those first:\n",
    "data_CNN['date'] = data_CNN['date'].str.replace(r'T.*Z', '', regex=True)\n",
    "\n",
    "# Convert to date-time format \n",
    "data_CNN['date'] = pd.to_datetime(data_CNN['date'], errors='coerce')\n",
    "# Convert to date-time format \n",
    "data_ABC['date'] = pd.to_datetime(data_ABC['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2653ff-594a-41ee-a089-cab3f49441ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_ABC, data_CNN], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9369415-e2a9-474c-8772-e93db564fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'article_date' column from oldest to newest\n",
    "data = data.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa94905-e64c-4b24-9aaf-1671b07623bc",
   "metadata": {},
   "source": [
    "## What is your document?\n",
    "\n",
    "Our document is the article as a whole - all text in the article excluding image discriptions, and authour tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e1489c-d80f-4450-a873-4397b38c17da",
   "metadata": {},
   "source": [
    "## Preprossesing\n",
    "- Clean text: ignore/remove any unwanted characters: casing, HTML markup, non-words, etc. (maybe also emoticons?)\n",
    "- Tokenization and stop-words\n",
    "- Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced0e70-1722-4900-bf16-9cb6364b30d1",
   "metadata": {},
   "source": [
    "***Removing NA values*** on the column \"article_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b7d9844-11fb-46e9-90a2-1da229699d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values in the 'article_text' column\n",
    "data = data.dropna(subset=['article_text'])\n",
    "data = data.reset_index(drop=True)  # Reset the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd19386-29dd-478d-8246-002923f8ecbb",
   "metadata": {},
   "source": [
    "***Cleaning the text***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9703aea0-bc14-4ea2-abf0-d6c6185c7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(document):\n",
    "    document = document.lower()  # To lower case\n",
    "    document = re.sub(r'<[^>]*>', ' ', document)  # Remove HTML\n",
    "    document = re.sub(r'[^\\w\\s&$€%]', '', document)  # Remove non-alphanumeric characters except &, $, %, and €\n",
    "    document = re.sub(r'&151', '', document)  # Remove specific string \"&151\"\n",
    "    return document\n",
    "\n",
    "data['article_text'] = data['article_text'].apply(cleaner)\n",
    "\n",
    "# Checking for duplicates: \n",
    "data[data['title'].duplicated()]\n",
    "data[data['article_text'].duplicated()]\n",
    "\n",
    "# Removing the duplicates articles: \n",
    "data = data.drop_duplicates(subset=['title']).reset_index(drop=True)\n",
    "data = data.drop_duplicates(subset=['article_text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b90da-6681-4e53-b344-b765cd9a2225",
   "metadata": {},
   "source": [
    "***Tokenization***\n",
    "- Splitting the articles into meaningfull elements to prepare for analysis. In our case we need to split the articles into words as these are what will be used for classifying sentiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c3f6ae-c133-4186-959b-529d3ea675e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'article_text' column into tokens based on whitespace and saving it into a new column\n",
    "# \"words\"\n",
    "data['words'] = data['article_text'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962fced-c116-4a20-a7ef-ee82c81e6c92",
   "metadata": {},
   "source": [
    "***Removing stop words***\n",
    "- These are words that occur very often and probably bear no useful information about the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b12e663-cf11-4546-bfc9-6d9128e99dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/astakettel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "data['words'] = [i for i in data['words'] if i not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666bb97-c08f-430a-bdea-51a1ed1db9d6",
   "metadata": {},
   "source": [
    "***Saving as a CSV file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "648b0c88-aa8d-4edb-9b08-9a24940458f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Users/astakettel/Desktop/ISDS/GitHub/ISDS-sentiment-analysis/EN_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
