{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5091a745-93af-4d40-99a4-d8c97ff97757",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6b60b6-8bbb-4c22-a7e5-916c81dfe495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ead705-c782-440e-a6f8-3fb5670cea8a",
   "metadata": {},
   "source": [
    "# Importing articles from ABCnews and CNN and merging them into one: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d92e63-f52d-4a81-9658-cc0fb3c9c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ABC = pd.read_csv('/Users/astakettel/Desktop/ISDS/GitHub/ISDS-sentiment-analysis/Webscraping/ABC_full_data.csv')\n",
    "\n",
    "data_CNN = pd.read_csv('/Users/astakettel/Desktop/ISDS/GitHub/ISDS-sentiment-analysis/Webscraping/CNN_full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb63dfdb-954f-4fec-98ed-c9f8c46961cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure we only keep relevant columns and giving them the same names:\n",
    "data_CNN = data_CNN[['headline', 'url', 'lastModifiedDate','article_text','source']]\n",
    "data_CNN = data_CNN.rename(columns={'headline': 'title', 'lastModifiedDate': 'date'})\n",
    "\n",
    "data_ABC = data_ABC.rename(columns = {'URL':'url', 'Title':'title', 'Date':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5d1a4-f43d-41b9-9b07-c329ee345ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the Date columns to be in the same format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8bf09d-c171-491b-8e62-a102a6f712f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CNN also has the time included, which confuses when trying to convert to datetime format, so we will remove those first:\n",
    "data_CNN['date'] = data_CNN['date'].str.replace(r'T.*Z', '', regex=True)\n",
    "\n",
    "# Convert to date-time format \n",
    "data_CNN['date'] = pd.to_datetime(data_CNN['date'], errors='coerce')\n",
    "# Convert to date-time format \n",
    "data_ABC['date'] = pd.to_datetime(data_ABC['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2653ff-594a-41ee-a089-cab3f49441ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_ABC, data_CNN], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9369415-e2a9-474c-8772-e93db564fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'article_date' column from oldest to newest\n",
    "data = data.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa94905-e64c-4b24-9aaf-1671b07623bc",
   "metadata": {},
   "source": [
    "## What is your document?\n",
    "\n",
    "Our document is the article as a whole - all text in the article excluding image discriptions, and authour tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e1489c-d80f-4450-a873-4397b38c17da",
   "metadata": {},
   "source": [
    "## Preprossesing\n",
    "- Clean text: ignore/remove any unwanted characters: casing, HTML markup, non-words, etc. (maybe also emoticons?)\n",
    "- Tokenization and stop-words\n",
    "- Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced0e70-1722-4900-bf16-9cb6364b30d1",
   "metadata": {},
   "source": [
    "***Removing NA values*** on the column \"article_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b7d9844-11fb-46e9-90a2-1da229699d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values in the 'article_text' column\n",
    "data = data.dropna(subset=['article_text'])\n",
    "data = data.reset_index(drop=True)  # Reset the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd19386-29dd-478d-8246-002923f8ecbb",
   "metadata": {},
   "source": [
    "***Cleaning the text***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9703aea0-bc14-4ea2-abf0-d6c6185c7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(document):\n",
    "    document = document.lower()  # To lower case\n",
    "    document = re.sub(r'<[^>]*>', ' ', document)  # Remove HTML\n",
    "    document = re.sub(r'[^\\w\\s&$€%]', '', document)  # Remove non-alphanumeric characters except &, $, %, and €\n",
    "    document = re.sub(r'&151', '', document)  # Remove specific string \"&151\"\n",
    "    return document\n",
    "\n",
    "data['article_text'] = data['article_text'].apply(cleaner)\n",
    "\n",
    "# Checking for duplicates: \n",
    "data[data['title'].duplicated()]\n",
    "data[data['article_text'].duplicated()]\n",
    "\n",
    "# Removing the duplicates articles: \n",
    "data = data.drop_duplicates(subset=['title']).reset_index(drop=True)\n",
    "data = data.drop_duplicates(subset=['article_text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b90da-6681-4e53-b344-b765cd9a2225",
   "metadata": {},
   "source": [
    "***Tokenization***\n",
    "- Splitting the articles into meaningfull elements to prepare for analysis. In our case we need to split the articles into words as these are what will be used for classifying sentiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c3f6ae-c133-4186-959b-529d3ea675e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'article_text' column into tokens based on whitespace and saving it into a new column\n",
    "# \"words\"\n",
    "data['words'] = data['article_text'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962fced-c116-4a20-a7ef-ee82c81e6c92",
   "metadata": {},
   "source": [
    "***Removing stop words***\n",
    "- These are words that occur very often and probably bear no useful information about the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b12e663-cf11-4546-bfc9-6d9128e99dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/astakettel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "data['words'] = [i for i in data['words'] if i not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666bb97-c08f-430a-bdea-51a1ed1db9d6",
   "metadata": {},
   "source": [
    "***Saving as a CSV file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "648b0c88-aa8d-4edb-9b08-9a24940458f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Users/astakettel/Desktop/ISDS/GitHub/ISDS-sentiment-analysis/EN_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4db37-14a2-4963-b68e-f2d485508ecf",
   "metadata": {},
   "source": [
    "# Using the lexicon vader for sentiment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276967d-bf22-4373-967d-7735a35319e3",
   "metadata": {},
   "source": [
    "## Computing sentiment scores from -1 to 1 and binary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "289202cf-1e29-468e-ad3b-bbeb17c51e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/astakettel/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        article_text  sentiment\n",
      "0    the end of the tv writers strike cleared the...     0.9851\n",
      "1    the end of the tv writers strike cleared the...     0.9892\n",
      "2    if the complex deal comes together in time a...     0.9674\n",
      "3    the companies which were ready to announce a...     0.7829\n",
      "4    delta air lines and northwest airlines on mo...     0.9684\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the VADER lexicon if you haven't already\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the VADER SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER sentiment analysis to each row and store the sentiment score in a new column\n",
    "def get_sentiment(text):\n",
    "    # Get the compound sentiment score\n",
    "    return analyser.polarity_scores(text)['compound']\n",
    "\n",
    "# Apply the function to the 'article_text'\n",
    "data['sentiment'] = data['article_text'].apply(get_sentiment)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(data[['article_text', 'sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09c9b4db-b9f4-4042-9582-6a6c68e27890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column using an if-else loop to classify sentiment\n",
    "data['pos/neg'] = ''\n",
    "\n",
    "# Loop through each row and assign the sentiment label\n",
    "for i in range(len(data)):\n",
    "    if data.loc[i, 'sentiment'] > 0:\n",
    "        data.loc[i, 'pos/neg'] = 'positive'\n",
    "    else:\n",
    "        data.loc[i, 'pos/neg'] = 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b89179-cbab-481d-8fe0-a42a608a7723",
   "metadata": {},
   "source": [
    "## Saving as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8844e40-5c00-413a-b4f3-ba1044f09514",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Users/astakettel/Desktop/ISDS/GitHub/ISDS-sentiment-analysis/Webscraping/EN_clean_sent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7fab5-7929-48c6-8b66-e677d0ad0866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ec874-30a2-4cf9-b210-c9850b6d01cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370145ef-8450-4d9a-baac-38365c35efd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07706d83-6ed8-4ea1-9293-b4a046d1f475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97225f-60c2-41ed-91c2-1bcfe037c2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
