{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5091a745-93af-4d40-99a4-d8c97ff97757",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e6b60b6-8bbb-4c22-a7e5-916c81dfe495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ead705-c782-440e-a6f8-3fb5670cea8a",
   "metadata": {},
   "source": [
    "# Importing articles from ABCnews and CNN and merging them into one: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d92e63-f52d-4a81-9658-cc0fb3c9c410",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_ABC \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mABC_full_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m data_CNN \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN_full_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "data_ABC = pd.read_csv('ABC_full_data.csv')\n",
    "\n",
    "data_CNN = pd.read_csv('CNN_full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb63dfdb-954f-4fec-98ed-c9f8c46961cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_CNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Making sure we only keep relevant columns and giving them the same names:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_CNN \u001b[38;5;241m=\u001b[39m data_CNN[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlastModifiedDate\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_text\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      3\u001b[0m data_CNN \u001b[38;5;241m=\u001b[39m data_CNN\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlastModifiedDate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      5\u001b[0m data_ABC \u001b[38;5;241m=\u001b[39m data_ABC\u001b[38;5;241m.\u001b[39mrename(columns \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_CNN' is not defined"
     ]
    }
   ],
   "source": [
    "# Making sure we only keep relevant columns and giving them the same names:\n",
    "data_CNN = data_CNN[['headline', 'url', 'lastModifiedDate','article_text','source']]\n",
    "data_CNN = data_CNN.rename(columns={'headline': 'title', 'lastModifiedDate': 'date'})\n",
    "\n",
    "data_ABC = data_ABC.rename(columns = {'URL':'url', 'Title':'title', 'Date':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5d1a4-f43d-41b9-9b07-c329ee345ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the Date columns to be in the same format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8bf09d-c171-491b-8e62-a102a6f712f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CNN also has the time included, which confuses when trying to convert to datetime format, so we will remove those first:\n",
    "data_CNN['date'] = data_CNN['date'].str.replace(r'T.*Z', '', regex=True)\n",
    "\n",
    "# Convert to date-time format \n",
    "data_CNN['date'] = pd.to_datetime(data_CNN['date'], errors='coerce')\n",
    "# Convert to date-time format \n",
    "data_ABC['date'] = pd.to_datetime(data_ABC['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2653ff-594a-41ee-a089-cab3f49441ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_ABC, data_CNN], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9369415-e2a9-474c-8772-e93db564fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'article_date' column from oldest to newest\n",
    "data = data.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa94905-e64c-4b24-9aaf-1671b07623bc",
   "metadata": {},
   "source": [
    "## What is your document?\n",
    "\n",
    "Our document is the article as a whole - all text in the article excluding image discriptions, and authour tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e1489c-d80f-4450-a873-4397b38c17da",
   "metadata": {},
   "source": [
    "## Preprossesing\n",
    "- Clean text: ignore/remove any unwanted characters: casing, HTML markup, non-words, etc. (maybe also emoticons?)\n",
    "- Tokenization and stop-words\n",
    "- Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced0e70-1722-4900-bf16-9cb6364b30d1",
   "metadata": {},
   "source": [
    "***Removing NA values*** on the column \"article_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b7d9844-11fb-46e9-90a2-1da229699d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values in the 'article_text' column\n",
    "data = data.dropna(subset=['article_text'])\n",
    "data = data.reset_index(drop=True)  # Reset the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd19386-29dd-478d-8246-002923f8ecbb",
   "metadata": {},
   "source": [
    "***Cleaning the text***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9703aea0-bc14-4ea2-abf0-d6c6185c7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(document):\n",
    "    document = document.lower()  # To lower case\n",
    "    document = re.sub(r'<[^>]*>', ' ', document)  # Remove HTML\n",
    "    document = re.sub(r'[^\\w\\s&$€%]', '', document)  # Remove non-alphanumeric characters except &, $, %, and €\n",
    "    document = re.sub(r'&151', '', document)  # Remove specific string \"&151\"\n",
    "    return document\n",
    "\n",
    "data['article_text'] = data['article_text'].apply(cleaner)\n",
    "\n",
    "# Checking for duplicates: \n",
    "data[data['title'].duplicated()]\n",
    "data[data['article_text'].duplicated()]\n",
    "\n",
    "# Removing the duplicates articles: \n",
    "data = data.drop_duplicates(subset=['title']).reset_index(drop=True)\n",
    "data = data.drop_duplicates(subset=['article_text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b90da-6681-4e53-b344-b765cd9a2225",
   "metadata": {},
   "source": [
    "***Tokenization***\n",
    "- Splitting the articles into meaningfull elements to prepare for analysis. In our case we need to split the articles into words as these are what will be used for classifying sentiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c3f6ae-c133-4186-959b-529d3ea675e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'article_text' column into tokens based on whitespace and saving it into a new column\n",
    "# \"words\"\n",
    "data['words'] = data['article_text'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962fced-c116-4a20-a7ef-ee82c81e6c92",
   "metadata": {},
   "source": [
    "***Removing stop words***\n",
    "- These are words that occur very often and probably bear no useful information about the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b12e663-cf11-4546-bfc9-6d9128e99dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/astakettel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "data['words'] = [i for i in data['words'] if i not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666bb97-c08f-430a-bdea-51a1ed1db9d6",
   "metadata": {},
   "source": [
    "***Saving as a CSV file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "648b0c88-aa8d-4edb-9b08-9a24940458f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Users/astakettel/Desktop/ISDS/GitHub/ISDS-sentiment-analysis/EN_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
